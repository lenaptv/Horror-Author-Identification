{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3b2bfa",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Train neural netwok to forecast horror author based on phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332952b",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab4614e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe037fc",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce45de8",
   "metadata": {},
   "source": [
    "Dataset consist of phrases of 3 horror authors: Edgar Po (EAP), HP Lovecraft (HPL), Mary Shelley (MWS)\n",
    "\n",
    "Dataset source - https://www.kaggle.com/competitions/spooky-author-identification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e550688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92ce81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1ecad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EAP  HPL  MWS\n",
       "0    1    0    0\n",
       "1    0    1    0\n",
       "2    1    0    0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## OneHotEncoding for y \n",
    "one_hot = pd.get_dummies(dataset['author'])\n",
    "one_hot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d56aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['text'].values\n",
    "y = dataset.join(one_hot)[['EAP','HPL','MWS']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d76288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set function for clear text from personal pronoun, punctuation, stopwords by spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ff93533",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "567c550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape:  (19579,)\n",
      "Processed 1000 out of 19579 documents.\n",
      "Processed 2000 out of 19579 documents.\n",
      "Processed 3000 out of 19579 documents.\n",
      "Processed 4000 out of 19579 documents.\n",
      "Processed 5000 out of 19579 documents.\n",
      "Processed 6000 out of 19579 documents.\n",
      "Processed 7000 out of 19579 documents.\n",
      "Processed 8000 out of 19579 documents.\n",
      "Processed 9000 out of 19579 documents.\n",
      "Processed 10000 out of 19579 documents.\n",
      "Processed 11000 out of 19579 documents.\n",
      "Processed 12000 out of 19579 documents.\n",
      "Processed 13000 out of 19579 documents.\n",
      "Processed 14000 out of 19579 documents.\n",
      "Processed 15000 out of 19579 documents.\n",
      "Processed 16000 out of 19579 documents.\n",
      "Processed 17000 out of 19579 documents.\n",
      "Processed 18000 out of 19579 documents.\n",
      "Processed 19000 out of 19579 documents.\n",
      "Cleaned up training data shape:  (19579,)\n"
     ]
    }
   ],
   "source": [
    "# clear text and check for completeness\n",
    "print('Original training data shape: ', X.shape)\n",
    "train_cleaned = cleanup_text(X, logging=True)\n",
    "print('Cleaned up training data shape: ', train_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31e6c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19374"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenezation and vectorization text data for training of model\n",
    "vocab_size = 25000 # max level of unique words \n",
    "max_len = 250 # max len for sentense \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_cleaned)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d067041",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ade6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(tokenized_train,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24ffd4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (15663, 250)\n",
      "X_test size: (3916, 250)\n",
      "y_train size: (15663, 3)\n",
      "y_test size: (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "print('X_train size: {}'.format(X_train.shape))\n",
    "print('X_test size: {}'.format(X_test.shape))\n",
    "print('y_train size: {}'.format(y_train.shape))\n",
    "print('y_test size: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957442b",
   "metadata": {},
   "source": [
    "## 2. Training of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9327c7",
   "metadata": {},
   "source": [
    "Recurrent neural networks based on LSTM (Long short-term memory) is more appropriate for such tasks, so select that type network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bb1b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dim = 32\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,emd_dim,input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=True)),\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd98a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d92fcc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 32)           800000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 250, 100)         33200     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 100)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 838,403\n",
      "Trainable params: 838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b55a427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "48/48 [==============================] - 159s 3s/step - loss: 1.0764 - accuracy: 0.4074 - val_loss: 1.0238 - val_accuracy: 0.4373\n",
      "Epoch 2/6\n",
      "48/48 [==============================] - 154s 3s/step - loss: 0.9303 - accuracy: 0.5561 - val_loss: 0.8726 - val_accuracy: 0.6195\n",
      "Epoch 3/6\n",
      "48/48 [==============================] - 160s 3s/step - loss: 0.7013 - accuracy: 0.7258 - val_loss: 0.6484 - val_accuracy: 0.7590\n",
      "Epoch 4/6\n",
      "48/48 [==============================] - 165s 3s/step - loss: 0.4300 - accuracy: 0.8433 - val_loss: 0.5373 - val_accuracy: 0.7922\n",
      "Epoch 5/6\n",
      "48/48 [==============================] - 166s 3s/step - loss: 0.2259 - accuracy: 0.9227 - val_loss: 0.5663 - val_accuracy: 0.7973\n",
      "Epoch 6/6\n",
      "48/48 [==============================] - 167s 3s/step - loss: 0.1440 - accuracy: 0.9542 - val_loss: 0.6357 - val_accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "#  model training\n",
    "batch_size = 264\n",
    "epochs = 6\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c30e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 95.42% / Validation accuracy: 79.80%\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % \n",
    "      (100*history.history['accuracy'][-1], 100*history.history['val_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149b70f",
   "metadata": {},
   "source": [
    "## 3. Check model on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6563e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 7s 47ms/step\n",
      "(3916, 3)\n"
     ]
    }
   ],
   "source": [
    "# prediction on test\n",
    "predicted_prob = model.predict(X_test)\n",
    "print(predicted_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "108c99da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.18% \n"
     ]
    }
   ],
   "source": [
    "# accuracy on test\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_accuracy.update_state(y_test, predicted_prob,\n",
    "               sample_weight=None)\n",
    "\n",
    "print(\"Test accuracy: %.2f%% \" % \n",
    "      (100*test_accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e1a20",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Accuracy on test at good level. Recurrent neural networks based on LSTM is efficient for that task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
